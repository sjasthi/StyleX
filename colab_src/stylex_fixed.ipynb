{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2LZuMBjrqCE",
   "metadata": {
    "id": "b2LZuMBjrqCE"
   },
   "source": [
    "#Questions/Tasks to Pursue\n",
    "\n",
    "1. StyleX doesn't work without CUDA/T4 GPU. Check for the processor and do not proceed if CUDA is not setup\n",
    "\n",
    "2. Perfecting the prompt - Play with multiple prompts to ensure that the output matches the input_images style. Perhaps, (1) generating multiple prompts and images and (2) comparing the outputs with the inputs - is the way to go.\n",
    "\n",
    "3. Need to dig deeper into STYLE_VOCAB and generate_one options.  What do these mean? How does it impact the image generation and final output?\n",
    "\n",
    "4. Is it even feasible to do batch generation (giving 50 prompts and generating 50 output images) with the free model?\n",
    "\n",
    "5. In the second cell, I tried to host the input_images on google_drive. But couldn't make it work! Something to explore and figure out how we can connect to google drive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xJYgEeRjqaXB",
   "metadata": {
    "id": "xJYgEeRjqaXB"
   },
   "source": [
    "#StyleX\n",
    "##Style-Conditioned Image Generation System\n",
    "\n",
    "### ICS499 - Software Engineering and Capstone Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc75263",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebc75263",
    "outputId": "5b331499-5f14-4e07-8bf6-7e58d854644a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#@title Install dependencies\n",
    "# in Colab (run once per new runtime)\n",
    "%pip -q install --upgrade diffusers transformers accelerate safetensors sentencepiece huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b776c",
   "metadata": {
    "id": "fb7b776c"
   },
   "outputs": [],
   "source": [
    "#@title Mount Google Drive  (TBD - Not working)\n",
    "# Optional: mount Google Drive for persistent input/output folders\n",
    "USE_GOOGLE_DRIVE = True\n",
    "DRIVE_PROJECT_PATH = '/content/drive/MyDrive/stylex'  # Change if your folder is elsewhere\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_PROJECT_ROOT = None\n",
    "if USE_GOOGLE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_PROJECT_ROOT = Path(DRIVE_PROJECT_PATH)\n",
    "    print('Drive project root:', DRIVE_PROJECT_ROOT)\n",
    "    print('Exists:', DRIVE_PROJECT_ROOT.exists())\n",
    "    if DRIVE_PROJECT_ROOT.exists():\n",
    "        print('Preview:', [x.name for x in DRIVE_PROJECT_ROOT.iterdir()][:10])\n",
    "    else:\n",
    "        print('Path not found. Update DRIVE_PROJECT_PATH and rerun this cell.')\n",
    "else:\n",
    "    print('Using local Colab runtime storage.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9157e1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9157e1f",
    "outputId": "43d0217a-5fda-4468-d235-8a91443df479"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
      "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /content\n",
      "Input dir: /content/input_images\n",
      "Output dir: /content/output_images\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "#@title import torch, diffusers and transformers\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "from huggingface_hub import whoami\n",
    "\n",
    "PROJECT_ROOT = DRIVE_PROJECT_ROOT if 'DRIVE_PROJECT_ROOT' in globals() and DRIVE_PROJECT_ROOT else Path('.').resolve()\n",
    "INPUT_ROOT = PROJECT_ROOT / 'input_images'\n",
    "OUTPUT_ROOT = PROJECT_ROOT / 'output_images'\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Project root:', PROJECT_ROOT)\n",
    "print('Input dir:', INPUT_ROOT)\n",
    "print('Output dir:', OUTPUT_ROOT)\n",
    "print('CUDA available:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e864c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a59e864c",
    "outputId": "071099b6-8d3b-416b-944e-46b41354a9be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF account: sjasthi\n"
     ]
    }
   ],
   "source": [
    "#@title Setup Hugging Face Credentials\n",
    "# Hugging Face login (required for gated models like stabilityai/stable-diffusion-3.5-medium)\n",
    "from huggingface_hub import login, notebook_login\n",
    "\n",
    "# Optional: paste token here, otherwise interactive login opens\n",
    "HF_TOKEN = 'Your token here'\n",
    "\n",
    "if HF_TOKEN.strip():\n",
    "    login(token=HF_TOKEN.strip(), add_to_git_credential=False)\n",
    "else:\n",
    "    notebook_login()\n",
    "\n",
    "try:\n",
    "    print('HF account:', whoami().get('name', '(unknown)'))\n",
    "except Exception:\n",
    "    print('Logged in, but could not verify account name.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db87986",
   "metadata": {
    "id": "9db87986"
   },
   "outputs": [],
   "source": [
    "#@title Setup the Style class\n",
    "@dataclass\n",
    "class Style:\n",
    "    name: str\n",
    "    folder: Path\n",
    "    prompt_suffix: str = ''\n",
    "    negative_prompt: str = ''\n",
    "    use_style_embeddings: bool = True\n",
    "    embeddings_top_k: int = 25\n",
    "    embeddings_model_id: str = 'openai/clip-vit-base-patch32'\n",
    "\n",
    "\n",
    "def list_styles(styles_root: Path) -> list[str]:\n",
    "    if not styles_root.exists():\n",
    "        return []\n",
    "    return sorted([p.name for p in styles_root.iterdir() if p.is_dir()])\n",
    "\n",
    "\n",
    "def _read_style_json(folder: Path) -> dict:\n",
    "    for fname in ('style.json', 'styles.json'):\n",
    "        p = folder / fname\n",
    "        if p.exists():\n",
    "            return json.loads(p.read_text(encoding='utf-8'))\n",
    "    return {}\n",
    "\n",
    "\n",
    "def load_style(styles_root: Path, style_name: str) -> Style:\n",
    "    folder = styles_root / style_name\n",
    "    if not folder.exists() or not folder.is_dir():\n",
    "        raise FileNotFoundError(f\"Style '{style_name}' not found in {styles_root}\")\n",
    "\n",
    "    data = _read_style_json(folder)\n",
    "\n",
    "    return Style(\n",
    "        name=style_name,\n",
    "        folder=folder,\n",
    "        prompt_suffix=data.get('prompt_suffix', ''),\n",
    "        negative_prompt=data.get('negative_prompt', ''),\n",
    "        use_style_embeddings=bool(data.get('use_style_embeddings', True)),\n",
    "        embeddings_top_k=int(data.get('embeddings_top_k', 10)),\n",
    "        embeddings_model_id=data.get('embeddings_model_id', 'openai/clip-vit-base-patch32'),\n",
    "    )\n",
    "\n",
    "\n",
    "def timestamp() -> str:\n",
    "    return time.strftime('%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31eebc",
   "metadata": {
    "id": "da31eebc"
   },
   "outputs": [],
   "source": [
    "#@title Setup the model\n",
    "IMG_EXTS = {'.png', '.jpg', '.jpeg', '.webp'}\n",
    "\n",
    "STYLE_VOCAB = [\n",
    "    'anime style', 'manga panel', 'cel shading', 'clean line art', 'vibrant colors',\n",
    "    'soft pastel colors', 'high contrast lighting', 'cinematic lighting', 'dramatic shadows',\n",
    "    'watercolor painting', 'oil painting', 'digital painting', 'concept art', '3d render',\n",
    "    'pixel art', 'low poly 3d', 'photorealistic', 'film grain', 'bokeh background',\n",
    "    'neon cyberpunk', 'futuristic cityscape', 'retro synthwave', 'comic book ink',\n",
    "    'studio ghibli inspired', 'minimalist illustration', 'flat vector art', 'highly detailed',\n",
    "    'intricate details', 'soft focus', 'sharp focus', 'dynamic composition',\n",
    "    'symmetrical composition', 'black and white', 'monochrome', 'manga style',\n",
    "    'painting style', 'drawing style', 'sketch style', 'fantasy art', 'sci-fi art',\n",
    "    'nature landscape', 'urban cityscape', 'soft lighting', 'hard lighting',\n",
    "    'volumetric lighting', 'golden hour lighting', 'moody lighting', 'studio lighting',\n",
    "    'rim lighting', 'backlit subject', 'foggy atmosphere', 'misty environment',\n",
    "    'dramatic sky', 'overcast lighting', 'professional photography', 'portrait photography',\n",
    "    'street photography', 'cinematic photography', 'depth of field',\n",
    "    'shallow depth of field', 'ultra realistic', 'high dynamic range', 'hdr photography',\n",
    "    'natural skin texture', 'semi realistic painting', 'stylized illustration',\n",
    "    'fantasy illustration', 'game concept art', 'character design',\n",
    "    'environment concept art', 'matte painting', 'detailed background',\n",
    "    'hand painted texture', 'realistic 3d render', 'cgi rendering',\n",
    "    'octane render style', 'unreal engine style', 'ray traced lighting',\n",
    "    'global illumination', 'subsurface scattering', 'high detail textures',\n",
    "    'gritty texture', 'smooth surfaces', 'weathered materials', 'metallic reflections',\n",
    "    'glass reflections', 'centered composition', 'rule of thirds composition',\n",
    "    'high resolution', 'extremely detailed', 'masterpiece quality',\n",
    "    'anime lighting', 'dynamic pose', 'expressive character', 'clean coloring',\n",
    "    'illustration style shading',\n",
    "]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StyleEmbeddingResult:\n",
    "    keywords: list[str]\n",
    "    scores: list[float]\n",
    "    used_images: list[str]\n",
    "\n",
    "\n",
    "def list_reference_images(style_folder: Path) -> list[Path]:\n",
    "    if not style_folder.exists():\n",
    "        return []\n",
    "    return sorted([p for p in style_folder.iterdir() if p.is_file() and p.suffix.lower() in IMG_EXTS])\n",
    "\n",
    "\n",
    "def _device_and_dtype(device: str) -> tuple[str, torch.dtype]:\n",
    "    use_cuda = device == 'cuda' and torch.cuda.is_available()\n",
    "    return ('cuda' if use_cuda else 'cpu'), (torch.float16 if use_cuda else torch.float32)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_style_keywords(\n",
    "    style_folder: Path,\n",
    "    device: str = 'cuda',\n",
    "    top_k: int = 20,\n",
    "    clip_model_id: str = 'openai/clip-vit-base-patch32',\n",
    ") -> StyleEmbeddingResult:\n",
    "    img_paths = list_reference_images(style_folder)\n",
    "    if not img_paths:\n",
    "        return StyleEmbeddingResult(keywords=[], scores=[], used_images=[])\n",
    "\n",
    "    dev, _dtype = _device_and_dtype(device)\n",
    "\n",
    "    model = CLIPModel.from_pretrained(clip_model_id).to(dev)\n",
    "    processor = CLIPProcessor.from_pretrained(clip_model_id)\n",
    "    if dev == 'cuda':\n",
    "        model = model.half()\n",
    "\n",
    "    imgs = [Image.open(p).convert('RGB') for p in img_paths]\n",
    "    img_inputs = processor(images=imgs, return_tensors='pt')\n",
    "    img_inputs = {k: v.to(dev) for k, v in img_inputs.items()}\n",
    "\n",
    "    vision_out = model.vision_model(pixel_values=img_inputs['pixel_values'], return_dict=True)\n",
    "    pooled = vision_out.pooler_output\n",
    "    image_features = model.visual_projection(pooled)\n",
    "\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    style_vec = image_features.mean(dim=0, keepdim=True)\n",
    "    style_vec = style_vec / style_vec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_inputs = processor(text=STYLE_VOCAB, return_tensors='pt', padding=True)\n",
    "    text_inputs = {k: v.to(dev) for k, v in text_inputs.items()}\n",
    "\n",
    "    text_out = model.text_model(\n",
    "        input_ids=text_inputs['input_ids'],\n",
    "        attention_mask=text_inputs.get('attention_mask', None),\n",
    "        return_dict=True,\n",
    "    )\n",
    "    text_pooled = text_out.pooler_output\n",
    "    text_features = model.text_projection(text_pooled)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    sims = (text_features @ style_vec.T).squeeze(1)\n",
    "    top_k = max(0, min(int(top_k), sims.numel()))\n",
    "    vals, idx = torch.topk(sims, k=top_k)\n",
    "\n",
    "    keywords = [STYLE_VOCAB[i] for i in idx.tolist()]\n",
    "    scores = [float(v) for v in vals.tolist()]\n",
    "    used = [p.name for p in img_paths]\n",
    "\n",
    "    return StyleEmbeddingResult(keywords=keywords, scores=scores, used_images=used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09d5f0",
   "metadata": {
    "id": "1e09d5f0"
   },
   "outputs": [],
   "source": [
    "#@title Building the prompt to the model\n",
    "def build_prompt(user_prompt: str, style: Style, style_keywords: list[str]) -> str:\n",
    "    parts = [user_prompt]\n",
    "    if style.prompt_suffix:\n",
    "        parts.append(style.prompt_suffix)\n",
    "    if style_keywords:\n",
    "        parts.append(', '.join(style_keywords))\n",
    "    return ', '.join([p for p in parts if p])\n",
    "\n",
    "\n",
    "def _load_style_cache(cache_path: Path) -> dict | None:\n",
    "    if not cache_path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(cache_path.read_text(encoding='utf-8'))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _save_style_cache(cache_path: Path, data: dict) -> None:\n",
    "    cache_path.write_text(json.dumps(data, indent=2), encoding='utf-8')\n",
    "\n",
    "\n",
    "def generate_one(\n",
    "    model_id: str,\n",
    "    user_prompt: str,\n",
    "    style: Style,\n",
    "    out_root: Path,\n",
    "    steps: int = 20,\n",
    "    guidance: float = 3.5,\n",
    "    height: int = 512,\n",
    "    width: int = 512,\n",
    "    device: str = 'cuda',\n",
    "    cpu_offload: bool = True,\n",
    "    no_t5: bool = True,\n",
    "):\n",
    "    use_cuda = device == 'cuda' and torch.cuda.is_available()\n",
    "    dtype = torch.float16 if use_cuda else torch.float32\n",
    "\n",
    "    style_keywords: list[str] = []\n",
    "    cache_file = style.folder / '.style_keywords_cache.json'\n",
    "\n",
    "    if style.use_style_embeddings:\n",
    "        cache = _load_style_cache(cache_file)\n",
    "        if cache and cache.get('top_k') == style.embeddings_top_k and cache.get('model_id') == style.embeddings_model_id:\n",
    "            style_keywords = cache.get('keywords', [])\n",
    "        else:\n",
    "            emb = extract_style_keywords(\n",
    "                style_folder=style.folder,\n",
    "                device='cuda' if use_cuda else 'cpu',\n",
    "                top_k=style.embeddings_top_k,\n",
    "                clip_model_id=style.embeddings_model_id,\n",
    "            )\n",
    "            style_keywords = emb.keywords\n",
    "            _save_style_cache(cache_file, {\n",
    "                'model_id': style.embeddings_model_id,\n",
    "                'top_k': style.embeddings_top_k,\n",
    "                'keywords': style_keywords,\n",
    "                'scores': emb.scores,\n",
    "                'used_images': emb.used_images,\n",
    "            })\n",
    "\n",
    "        print('Embedding style keywords:', style_keywords)\n",
    "\n",
    "    pipe_kwargs = dict(torch_dtype=dtype)\n",
    "    if no_t5:\n",
    "        pipe_kwargs['text_encoder_3'] = None\n",
    "        pipe_kwargs['tokenizer_3'] = None\n",
    "\n",
    "    try:\n",
    "        pipe = StableDiffusion3Pipeline.from_pretrained(model_id, **pipe_kwargs)\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if 'gated' in msg.lower() or '401' in msg or 'unauthorized' in msg.lower():\n",
    "            raise RuntimeError(\n",
    "                f\"Cannot access model '{model_id}'. It is likely gated on Hugging Face.Run the HF login cell, request access to the model page, then rerun.\"\n",
    "            ) from e\n",
    "        raise\n",
    "    pipe.enable_attention_slicing()\n",
    "    if hasattr(pipe, 'vae'):\n",
    "        pipe.vae.enable_slicing()\n",
    "        pipe.vae.enable_tiling()\n",
    "\n",
    "    if cpu_offload:\n",
    "        pipe.enable_model_cpu_offload()\n",
    "    else:\n",
    "        pipe = pipe.to('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "    prompt = build_prompt(user_prompt, style, style_keywords)\n",
    "    print('Final prompt:', prompt)\n",
    "\n",
    "    call_kwargs = dict(\n",
    "        num_inference_steps=steps,\n",
    "        guidance_scale=guidance,\n",
    "        height=height,\n",
    "        width=width,\n",
    "    )\n",
    "\n",
    "    negative_prompt = getattr(style, 'negative_prompt', '') or ''\n",
    "    if negative_prompt:\n",
    "        call_kwargs['negative_prompt'] = negative_prompt\n",
    "\n",
    "    result = pipe(prompt, **call_kwargs)\n",
    "    images = result.images\n",
    "\n",
    "    out_dir = out_root / style.name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ts = timestamp()\n",
    "    out_paths = []\n",
    "    for i, img in enumerate(images):\n",
    "        out_path = out_dir / f'{ts}_{style.name}_{i}.png'\n",
    "        img.save(out_path)\n",
    "        out_paths.append(out_path)\n",
    "\n",
    "    return out_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaceb23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477,
     "referenced_widgets": [
      "b89c766d22934552bc61325ca80ae05a",
      "c35c69190c5f4a80a67fe16e82e712b8",
      "2483ce2a45ab4c18b91af911690f8ed5",
      "785ded2a3f8946bc9a4281dbb3f79a3f",
      "f8a5e0c12ffa4b9093f71c5826a92837",
      "6221bdf4d25741d8bbcd9e038c04badd",
      "0c060fada5694699bcde2592da2da068",
      "834762a3d7d743aeb44770fa8555f84d",
      "0c142def94c84bdd9d907177e59a49d5",
      "c51b833e92a44ae9b7789da5105a706b",
      "37118efe7d324a9093ce1fd6ff3ca04a",
      "4d43ca7f9ad34affbdf429f0c3485ca9",
      "a1ca3b9fac5541f3966ae0d4ebc08ca5",
      "f178579ecff245ca992578a0ade3d783",
      "181e6533808740ce9227f21d5bfeefb0",
      "156d1a243b624c65bf73a7b7e988e402",
      "6f277219ff084ac08f637da29f0c56b4",
      "c7384efb681541b08f7f13f2fa0c77d0",
      "99d289d7653149ba9f23858336e827d5",
      "b444dda2bd8c43dcad9d235ff0cd7db8",
      "4bf55e36af78488cb9e0051259b14d73",
      "788e96295d024640af75377b07d9fa9f",
      "a829ee6737d64ea7830a547ee9089206",
      "9cc0634dd08e491c8ded8fdaf8a7933d",
      "e5a6e75facc6436f82076e6a781b3377",
      "c91cf71ede04411386141b076b76e076",
      "e3a0f751104a4c1bafbd89ef7a7af23b",
      "86b54026a13249e5aba9e43740adac54",
      "5950f97a7b4d4bcda30381806748217c",
      "d097f8dad1254a329b65a4f4b00fb49a",
      "e21d500cb86e4cf89ebd06585cd2b10d",
      "749b7ef5290646ed87afc7eecb40a740",
      "d286d901367a425d9ffcce9cb279c390",
      "4dbfe940dafd4f1084d831daf8bc360b",
      "e855e631afcb4f27a13e1290a7473133",
      "f427c34014d447fd829cd719a04a560d",
      "4626c53c3670440db6ce03cd3e96f778",
      "f77c06b5c5064893851968436a5da5af",
      "ea6acd3f7e5345d9bb09c8bb4d214895",
      "6c3cbb4b6a3749b49799fe72fd80816e",
      "77864d623adc4ae7a089cc29228fce1c",
      "d9762969bdfc4a8fa06971768d8a7a1f",
      "3bde5e492e694d57bb80abc8f1fcfb73",
      "8be7f9d5b024488ba53a2eab1c5384fb"
     ]
    },
    "id": "baaceb23",
    "outputId": "91b8e05b-ac8b-4fab-ec91-432bc979185c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available styles: ['.ipynb_checkpoints', 'babu']\n",
      "Using style: babu\n",
      "Style folder: /content/input_images/babu\n",
      "Files in style folder: 15\n",
      " - 07 Garad Saree.jpg\n",
      " - 06 Maharashtra Nauvari.jpg\n",
      " - 14 Lehenga Choli.jpg\n",
      " - 05 Kerala Kasavu.jpg\n",
      " - 11 Coorgi Saree.jpg\n",
      " - .style_keywords_cache.json\n",
      " - 13 Bihu Dress.jpg\n",
      " - 15 Goa Koli Dress.jpg\n",
      " - 08 Madisar Kattu.jpg\n",
      " - 10 Lambada dress.jpg\n",
      "Embedding style keywords: ['stylized illustration', 'illustration style shading', 'flat vector art', 'character design', 'pixel art', 'expressive character', 'drawing style', 'clean coloring', 'painting style', 'sketch style']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89c766d22934552bc61325ca80ae05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d43ca7f9ad34affbdf429f0c3485ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a829ee6737d64ea7830a547ee9089206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prompt: Indian woman researcher in machine learning presenting neural network diagrams, anime illustration, clean line art, vibrant colors, soft lighting, expressive character design, in the visual style of the provided reference images, stylized illustration, illustration style shading, flat vector art, character design, pixel art, expressive character, drawing style, clean coloring, painting style, sketch style\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbfe940dafd4f1084d831daf8bc360b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - /content/output_images/babu/20260226_044435_babu_0.png\n"
     ]
    }
   ],
   "source": [
    "#@title Take User inputs\n",
    "\n",
    "#TBD: Experiment with different prompts to get closer to the input_images\n",
    "\n",
    "PROMPT = \"Portrait of an Indian woman AI and ML scientist at her workstation, digital painting, soft brush strokes, teal-orange palette, cinematic rim lighting, highly detailed, in the visual style of the provided reference images\"\n",
    "\n",
    "PROMPT = \"Indian woman researcher in machine learning presenting neural network diagrams, anime illustration, clean line art, vibrant colors, soft lighting, expressive character design, in the visual style of the provided reference images\"\n",
    "\n",
    "# Must match a folder name under input_images/\n",
    "STYLE_NAME = 'first'\n",
    "MODEL_ID = 'stabilityai/stable-diffusion-3.5-medium'\n",
    "\n",
    "# Increase the steps for better results (25 to 40)\n",
    "STEPS = 30\n",
    "\n",
    "GUIDANCE = 3.5\n",
    "HEIGHT = 512\n",
    "WIDTH = 512\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CPU_OFFLOAD = True\n",
    "\n",
    "#Set the following to False for better prompt understanding/uses more memory\n",
    "NO_T5 = True\n",
    "\n",
    "available = list_styles(INPUT_ROOT)\n",
    "print('Available styles:', available)\n",
    "if STYLE_NAME not in available:\n",
    "    raise ValueError(f\"Unknown style '{STYLE_NAME}'. Available: {available}\")\n",
    "\n",
    "style = load_style(INPUT_ROOT, STYLE_NAME)\n",
    "style_files = [p for p in style.folder.glob('*.*')]\n",
    "print('Using style:', style.name)\n",
    "print('Style folder:', style.folder)\n",
    "print('Files in style folder:', len(style_files))\n",
    "for p in style_files[:10]:\n",
    "    print(' -', p.name)\n",
    "\n",
    "paths = generate_one(\n",
    "    model_id=MODEL_ID,\n",
    "    user_prompt=PROMPT,\n",
    "    style=style,\n",
    "    out_root=OUTPUT_ROOT,\n",
    "    steps=STEPS,\n",
    "    guidance=GUIDANCE,\n",
    "    height=HEIGHT,\n",
    "    width=WIDTH,\n",
    "    device=DEVICE,\n",
    "    cpu_offload=CPU_OFFLOAD,\n",
    "    no_t5=NO_T5,\n",
    ")\n",
    "\n",
    "print('Saved:')\n",
    "for p in paths:\n",
    "    print(' -', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adccd5fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "adccd5fa",
    "outputId": "3d0cea24-a9e5-4f3d-cae9-fd1b30bc56ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created zip: /content/output_images.zip\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_efb9057c-6c99-4208-8cac-78b692093477\", \"output_images.zip\", 19983838)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Download the output images\n",
    "# Zip + download all generated images from output_images\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "zip_base = str(PROJECT_ROOT / 'output_images')\n",
    "zip_path = shutil.make_archive(zip_base, 'zip', root_dir=OUTPUT_ROOT)\n",
    "print('Created zip:', zip_path)\n",
    "files.download(zip_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
